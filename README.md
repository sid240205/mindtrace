# üß† MindTrace

> **AI-Powered Memory Assistant for Smart Glasses**

[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![React 19](https://img.shields.io/badge/React-19-61DAFB?style=flat-square&logo=react&logoColor=black)](https://react.dev)
[![Vite](https://img.shields.io/badge/Vite-7.2-646CFF?style=flat-square&logo=vite&logoColor=white)](https://vitejs.dev)
[![TailwindCSS](https://img.shields.io/badge/TailwindCSS-4.1-38B2AC?style=flat-square&logo=tailwind-css&logoColor=white)](https://tailwindcss.com)
[![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-2.0-red?style=flat-square)](https://www.sqlalchemy.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-13%2B-336791?style=flat-square&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-0.5-orange?style=flat-square)](https://www.trychroma.com/)
[![InsightFace](https://img.shields.io/badge/InsightFace-ArcFace-purple?style=flat-square)](https://github.com/deepinsight/insightface)
[![Whisper](https://img.shields.io/badge/OpenAI-Whisper-412991?style=flat-square&logo=openai&logoColor=white)](https://github.com/openai/whisper)
[![Gemini](https://img.shields.io/badge/Google-Gemini_2.5-4285F4?style=flat-square&logo=google&logoColor=white)](https://ai.google.dev/)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen?style=flat-square)](http://makeapullrequest.com)

---

## üìñ Overview

MindTrace is a production-ready AI memory assistant designed for Ray-Ban Meta smart glasses and similar wearable devices. It combines real-time face recognition, live speech transcription, and context-aware AI assistance to help users‚Äîparticularly those with memory challenges‚Äînavigate social interactions with confidence.

### Core Capabilities

- **Real-Time Face Recognition**: Instant identification using InsightFace with ArcFace embeddings
- **Live Speech-to-Text**: Continuous transcription via OpenAI Whisper with WebRTC VAD
- **Context-Aware AI Assistant**: Google Gemini-powered chat with full access to user data
- **Vector Search**: ChromaDB for semantic search across conversations and face embeddings
- **Emergency SOS System**: One-touch alerts with GPS location sharing
- **Smart Reminders**: Medication, meal, and activity scheduling with notifications
- **Comprehensive Dashboard**: Web-based management interface for caregivers and users

---

## üéØ Key Features

### üëÅÔ∏è Real-Time Face Recognition

**Technology Stack:**
- **Detection**: RetinaFace (InsightFace buffalo_l model)
- **Embedding**: ArcFace (512-dimensional face embeddings)
- **Storage**: ChromaDB with cosine similarity search
- **Threshold**: 0.45 similarity score for positive identification

**How It Works:**
1. Camera captures frame from smart glasses
2. RetinaFace detects all faces with bounding boxes
3. ArcFace generates 512-dim embeddings for each face
4. ChromaDB performs vector similarity search against stored contacts
5. Results streamed back to HUD overlay with name, relationship, and confidence

**Performance:**
- Detection: ~30-50ms per frame (640x640 input)
- Recognition: ~10-20ms per face via ChromaDB
- End-to-end latency: <100ms

### üéôÔ∏è Live Speech-to-Text

**Technology Stack:**
- **ASR Model**: OpenAI Whisper (base model, English)
- **VAD**: WebRTC Voice Activity Detection (aggressiveness=2)
- **Streaming**: WebSocket with 30ms frame duration
- **Sample Rate**: 16kHz mono audio

**Pipeline:**
1. Audio captured from smart glasses microphone
2. WebRTC VAD filters non-speech frames
3. Speech segments buffered and sent to Whisper
4. Transcriptions streamed to HUD in real-time
5. Full conversations stored in ChromaDB for semantic search

**Accuracy:**
- Word Error Rate (WER): ~5-10% for clear speech
- Latency: ~200-500ms per utterance
- Supports continuous conversation tracking

### ü§ñ Context-Aware AI Assistant

**Technology Stack:**
- **Model**: Google Gemini 2.5 Flash Lite
- **Context Window**: 2048 tokens output, ~10 message history
- **Temperature**: 0.7 for balanced creativity/accuracy
- **Integration**: Full access to PostgreSQL + ChromaDB data

**Available Context:**
- User profile and preferences
- All contacts with relationships and last seen dates
- Active reminders and schedules
- Recent interactions and conversation summaries
- Emergency contacts and SOS settings
- System alerts and notifications

**Capabilities:**
- Natural language queries about contacts ("Who is my daughter?")
- Reminder management ("What medications do I take today?")
- Feature navigation ("How do I add an emergency contact?")
- Conversation insights ("What did I discuss with John last week?")

### üóÑÔ∏è ChromaDB Vector Database

**Collections:**

1. **Faces Collection**
   - **Purpose**: Store and search face embeddings
   - **Embedding**: Pre-computed 512-dim ArcFace vectors
   - **Distance Metric**: Cosine similarity
   - **Metadata**: contact_id, user_id, name, relationship
   - **Index**: HNSW (Hierarchical Navigable Small World)

2. **Conversations Collection**
   - **Purpose**: Semantic search across interaction history
   - **Embedding**: all-MiniLM-L6-v2 (384-dim, auto-generated)
   - **Distance Metric**: Cosine similarity
   - **Metadata**: interaction_id, user_id, contact_id, timestamp, mood
   - **Use Cases**: "Find conversations about medication", "What did we discuss about travel?"

**Configuration:**
```python
# server/.env
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_TENANT=default_tenant
CHROMA_DATABASE=default_database
CHROMA_API_KEY=optional_for_cloud
```

**Deployment Options:**
- **Local**: In-memory or persistent file storage
- **Server**: Self-hosted ChromaDB server
- **Cloud**: Hosted at api.trychroma.com with authentication

### üö® Emergency SOS System

**Features:**
- One-button emergency activation from smart glasses
- Real-time GPS location sharing via browser geolocation API
- Configurable alert preferences (SMS, calls, email)
- Priority-based contact notification
- SOS alert history and management
- Automatic location updates every 30 seconds during active SOS

**Alert Flow:**
1. User triggers SOS from glasses or dashboard
2. System captures GPS coordinates
3. Alerts sent to emergency contacts by priority
4. Location shared via secure link
5. Caregivers can view real-time location on map
6. User can cancel SOS when safe

### üìä Comprehensive Dashboard

**Pages:**
- **Dashboard Home**: Overview with quick stats and recent activity
- **Contacts Directory**: Manage contacts with face photo enrollment
- **Interaction History**: Review past conversations with summaries
- **Reminders**: Configure medication, meal, and activity alerts
- **SOS Settings**: Manage emergency contacts and preferences
- **Alerts**: View and manage system notifications
- **AI Chat**: Context-aware assistant with full data access
- **Profile Settings**: User preferences and account management

---

## üèóÔ∏è Architecture

### System Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Smart Glasses                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ   Camera     ‚îÇ  ‚îÇ  Microphone  ‚îÇ  ‚îÇ  GPS/Sensors ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ                  ‚îÇ
          ‚ñº                  ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Glass Client (React)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  HUD Overlay: Face labels, Transcriptions, Alerts       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ HTTP/WebSocket
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FastAPI Server (Python)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ Face Routes  ‚îÇ  ‚îÇ  ASR Routes  ‚îÇ  ‚îÇ  Chat Routes ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ                  ‚îÇ
‚îÇ         ‚ñº                  ‚ñº                  ‚ñº                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ InsightFace  ‚îÇ  ‚îÇ   Whisper    ‚îÇ  ‚îÇ    Gemini    ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  (ArcFace)   ‚îÇ  ‚îÇ  + WebRTC    ‚îÇ  ‚îÇ  2.5 Flash   ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ                  ‚îÇ
          ‚ñº                  ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Data Layer                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL  ‚îÇ  ‚îÇ   ChromaDB   ‚îÇ  ‚îÇ  File Store  ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  (Metadata)  ‚îÇ  ‚îÇ  (Vectors)   ‚îÇ  ‚îÇ   (Photos)   ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñ≤
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Dashboard Client (React)                          ‚îÇ
‚îÇ  Contact Management ‚îÇ Reminders ‚îÇ SOS ‚îÇ Chat ‚îÇ History          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technology Stack

#### Backend (Python)
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Web Framework** | FastAPI | 0.123+ | REST API and WebSocket server |
| **ASGI Server** | Uvicorn | 0.38+ | Production-grade async server |
| **Database ORM** | SQLAlchemy | 2.0+ | PostgreSQL/SQLite abstraction |
| **Database** | PostgreSQL / SQLite | 13+ / 3.x | Relational data storage |
| **Face Recognition** | InsightFace (buffalo_l) | Latest | RetinaFace + ArcFace models |
| **Computer Vision** | OpenCV | 4.9 | Image processing |
| **Speech-to-Text** | OpenAI Whisper | base | Audio transcription (74M params) |
| **Voice Activity** | WebRTC VAD | 2.0+ | Speech detection |
| **AI Assistant** | Google Gemini | 2.5 Flash Lite | Context-aware chat |
| **Vector Database** | ChromaDB | 0.5+ | Embedding storage and search |
| **Text Embeddings** | all-MiniLM-L6-v2 | - | Sentence transformers (384-dim) |
| **ML Framework** | ONNX Runtime | 1.23+ | Model inference |
| **Authentication** | JWT + OAuth (Authlib) | 1.6+ | Secure user sessions |
| **Password Hashing** | bcrypt | 4.0+ | Secure credential storage |
| **HTTP Client** | httpx + requests | Latest | External API calls |
| **Environment** | python-dotenv | 1.2+ | Configuration management |
| **Data Validation** | Pydantic | 2.12+ | Request/response schemas |
| **Image Processing** | Pillow | 12.0 | Image manipulation |
| **Scientific Computing** | NumPy, SciPy | Latest | Numerical operations |
| **ML Utilities** | scikit-learn, scikit-image | Latest | Data processing |

#### Frontend (JavaScript/TypeScript)
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Framework** | React | 19.2 | UI components and state management |
| **Build Tool** | Vite | 7.2+ | Fast development and bundling |
| **Routing** | React Router | 7.10+ | Client-side navigation |
| **Styling** | TailwindCSS | 4.1+ | Utility-first CSS framework |
| **HTTP Client** | Axios | 1.13+ | API communication |
| **Icons** | Lucide React | 0.555+ | Icon library |
| **Maps** | Leaflet + React Leaflet | 1.9+ / 5.0+ | GPS visualization |
| **Notifications** | React Hot Toast | 2.6+ | User feedback and toasts |
| **Linting** | ESLint | 9.39+ | Code quality |
| **Package Manager** | npm | Latest | Dependency management |

#### AI/ML Models
| Model | Version | Parameters | Use Case | Latency |
|-------|---------|-----------|----------|---------|
| **RetinaFace** | buffalo_l | - | Face detection | ~30-50ms |
| **ArcFace** | buffalo_l | 512-dim | Face embedding | ~10ms |
| **Whisper** | base | 74M | Speech-to-text | ~200-500ms |
| **Gemini** | 2.5 Flash Lite | - | AI chat | ~500-1500ms |
| **MiniLM** | L6-v2 | 22M | Text embedding (ChromaDB) | ~50ms |

---

## üöÄ Quick Start

### Prerequisites

- **Node.js** 18+ and npm
- **Python** 3.10-3.12
- **uv** (Python package manager)
- **PostgreSQL** 13+ (optional, SQLite by default)
- **ChromaDB** server (optional, in-memory by default)

### Installation

```bash
# 1. Clone repository
git clone https://github.com/yourusername/mindtrace.git
cd mindtrace

# 2. Install uv (if not installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 3. Setup server
cd server
uv sync
cp .env.example .env
# Edit .env with your API keys

# 4. Setup dashboard
cd ../client
npm install
cp .env.example .env

# 5. Setup glass client (optional)
cd ../glass-client
npm install
cp .env.example .env
```

### Configuration

#### Server Environment Variables (`server/.env`)

```env
# Server Configuration
PORT=8000
CLIENT_URL=http://localhost:5173
GLASS_URL=http://localhost:5174
SECRET_KEY=your-secret-key-here-min-32-chars

# AI Services (Required)
GEMINI_API_KEY=your-gemini-api-key-here

# Database (Optional - defaults to SQLite)
DATABASE_URL=postgresql://user:password@localhost:5432/mindtrace

# ChromaDB (Optional - defaults to in-memory)
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_TENANT=default_tenant
CHROMA_DATABASE=default_database
CHROMA_API_KEY=optional-for-cloud-deployment

# Hugging Face (Optional - for model downloads)
HF_TOKEN=your-huggingface-token
```

#### Client Environment Variables (`client/.env`)

```env
VITE_BASE_URL=http://localhost:8000
```

#### Glass Client Environment Variables (`glass-client/.env`)

```env
VITE_API_BASE_URL=http://localhost:8000
```

### Running the Application

```bash
# Terminal 1: Start server
cd server
uv run main.py
# Server runs at http://localhost:8000

# Terminal 2: Start dashboard
cd client
npm run dev
# Dashboard runs at http://localhost:5173

# Terminal 3: Start glass client (optional)
cd glass-client
npm run dev
# Glass HUD runs at http://localhost:5174
```

### First-Time Setup

1. Open dashboard at `http://localhost:5173`
2. Create an account or login
3. Navigate to **Contacts Directory**
4. Add contacts with profile photos
5. Click **Sync Faces** to generate embeddings
6. Configure reminders and SOS contacts
7. Test face recognition from glass client

---

## üì¶ Project Structure

```
mindtrace/
‚îú‚îÄ‚îÄ server/                          # FastAPI Backend
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # Application entry point
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml               # uv project configuration
‚îÇ   ‚îú‚îÄ‚îÄ .env                         # Environment variables
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ app/                         # Application code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py                   # FastAPI app setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py                # SQLAlchemy database models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chroma_client.py         # ChromaDB client singleton
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/                  # API endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ authRoutes.py        # Authentication (JWT + OAuth)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faceRoutes.py        # Face recognition API
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contactRoutes.py     # Contact management
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chatRoutes.py        # AI chat (Gemini)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ asrRoutes.py         # Speech-to-text WebSocket
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sosRoutes.py         # Emergency SOS
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reminderRoutes.py    # Reminder management
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alertRoutes.py       # Alert system
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ interactionRoutes.py # Interaction history
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/                # Business logic
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ai_service.py        # Gemini AI integration
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ai_engine/                   # ML Models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ face_engine.py           # InsightFace (RetinaFace + ArcFace)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ asr/                     # Speech-to-text
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ asr_engine.py        # Whisper integration
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ vad_engine.py        # WebRTC VAD
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ conversation_store.py # Conversation persistence
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/                        # Data storage
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ conversations.json       # Conversation backup (JSON)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ sync_faces.py                # CLI: Sync face embeddings to ChromaDB
‚îÇ   ‚îú‚îÄ‚îÄ verify_asr.py                # CLI: Test ASR engine
‚îÇ   ‚îî‚îÄ‚îÄ verify_engine.py             # CLI: Test face recognition
‚îÇ
‚îú‚îÄ‚îÄ client/                          # Dashboard (React + Vite)
‚îÇ   ‚îú‚îÄ‚îÄ package.json                 # Node dependencies
‚îÇ   ‚îú‚îÄ‚îÄ vite.config.js               # Vite configuration
‚îÇ   ‚îú‚îÄ‚îÄ index.html                   # HTML entry point
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ main.jsx                 # React entry point
‚îÇ       ‚îú‚îÄ‚îÄ App.jsx                  # Root component with routing
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ pages/                   # Route pages
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ DashboardHome.jsx    # Dashboard overview
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ContactsDirectory.jsx # Contact management
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ InteractionHistory.jsx # Conversation history
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Reminders.jsx        # Reminder management
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ SOSSettings.jsx      # Emergency contacts
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Alerts.jsx           # System alerts
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ AIChat.jsx           # AI assistant
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Profile.jsx          # User settings
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ components/              # Reusable components
‚îÇ           ‚îú‚îÄ‚îÄ ContactAvatar.jsx    # Contact display
‚îÇ           ‚îú‚îÄ‚îÄ ReminderCard.jsx     # Reminder UI
‚îÇ           ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ glass-client/                    # Smart Glasses HUD (React + Vite)
    ‚îú‚îÄ‚îÄ package.json                 # Node dependencies
    ‚îú‚îÄ‚îÄ vite.config.js               # Vite configuration
    ‚îú‚îÄ‚îÄ vercel.json                  # Deployment config
    ‚îÇ
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ pages/
        ‚îÇ   ‚îî‚îÄ‚îÄ FaceRecognition.jsx  # Main HUD page
        ‚îÇ
        ‚îî‚îÄ‚îÄ components/
            ‚îî‚îÄ‚îÄ HUDOverlay.jsx       # Face labels and transcriptions
```

---

## üîß API Documentation

### Face Recognition

#### POST `/face/recognize`
Upload image frame for face recognition.

**Request:**
```json
{
  "image": "base64_encoded_image_data",
  "user_id": 1
}
```

**Response:**
```json
{
  "faces": [
    {
      "name": "John Doe",
      "relation": "Brother",
      "confidence": 0.87,
      "bbox": [100, 150, 300, 400],
      "contact_id": 42
    }
  ],
  "count": 1
}
```

#### POST `/face/sync`
Sync face embeddings from database to ChromaDB.

**Response:**
```json
{
  "success": true,
  "count": 15,
  "message": "Successfully synced 15 face embeddings"
}
```

### Speech-to-Text

#### WebSocket `/asr/{profile_id}`
Real-time speech transcription.

**Client ‚Üí Server:**
```json
{
  "audio": "base64_encoded_audio_chunk",
  "sample_rate": 16000
}
```

**Server ‚Üí Client:**
```json
{
  "type": "transcription",
  "text": "Hello, how are you today?",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

### AI Chat

#### POST `/chat/stream`
Stream AI assistant response.

**Request:**
```json
{
  "message": "Who is my daughter?",
  "conversation_history": [
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi! How can I help?"}
  ]
}
```

**Response:** Server-Sent Events (SSE)
```
data: {"chunk": "Your daughter is "}
data: {"chunk": "Emily Johnson. "}
data: {"chunk": "She lives in Seattle."}
data: [DONE]
```

### Contacts

#### GET `/contacts`
List all contacts for authenticated user.

**Response:**
```json
{
  "contacts": [
    {
      "id": 1,
      "name": "Emily Johnson",
      "relationship": "family",
      "relationship_detail": "Daughter",
      "phone_number": "+1234567890",
      "email": "emily@example.com",
      "last_seen": "2024-01-15T10:00:00Z",
      "notes": "Lives in Seattle, works as a teacher",
      "has_photo": true
    }
  ]
}
```

#### POST `/contacts`
Create new contact with optional photo.

**Request:** multipart/form-data
```
name: "John Smith"
relationship: "friend"
phone_number: "+1987654321"
photo: <file>
```

---

## üß™ Testing & Verification

### Test Face Recognition

```bash
cd server

# Sync faces from database to ChromaDB
uv run sync_faces.py

# Test face recognition engine
uv run verify_engine.py
```

### Test Speech-to-Text

```bash
cd server

# Test Whisper ASR with microphone
uv run verify_asr.py
```

### Test API Endpoints

```bash
# Health check
curl http://localhost:8000/health

# Test face recognition (requires auth token)
curl -X POST http://localhost:8000/face/recognize \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"image": "base64_data", "user_id": 1}'
```

---

## üö¢ Deployment

### Production Checklist

- [ ] Set strong `SECRET_KEY` (min 32 characters)
- [ ] Configure production database (PostgreSQL recommended)
- [ ] Setup ChromaDB server or cloud instance
- [ ] Enable HTTPS/SSL for all endpoints
- [ ] Configure CORS for production domains
- [ ] Set up monitoring and logging
- [ ] Enable rate limiting on API endpoints
- [ ] Configure backup strategy for database and ChromaDB
- [ ] Test emergency SOS system end-to-end
- [ ] Verify face recognition accuracy with test dataset

### Docker Deployment (Coming Soon)

```bash
# Build and run with Docker Compose
docker-compose up -d
```

### Cloud Deployment

**Server Options:**
- AWS EC2 + RDS (PostgreSQL) + S3 (photos)
- Google Cloud Run + Cloud SQL + Cloud Storage
- DigitalOcean App Platform + Managed PostgreSQL

**Client Options:**
- Vercel (recommended for React apps)
- Netlify
- AWS Amplify
- Cloudflare Pages

---

## ü§ù Contributing

We welcome contributions! Please follow these guidelines:

### Development Workflow

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes with clear commit messages
4. Test thoroughly (face recognition, ASR, API endpoints)
5. Update documentation if needed
6. Submit a pull request

### Code Style

**Python:**
- Follow PEP 8 style guide
- Use type hints for function signatures
- Add docstrings for public functions
- Run `black` for formatting

**JavaScript/React:**
- Follow ESLint configuration
- Use functional components with hooks
- Keep components small and focused
- Add PropTypes or TypeScript types

### Commit Messages

Use conventional commit format:
```
feat: Add voice command activation
fix: Resolve ChromaDB connection timeout
docs: Update API documentation
refactor: Optimize face recognition pipeline
test: Add unit tests for ASR engine
```

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

### Open Source Projects
- [InsightFace](https://github.com/deepinsight/insightface) - Face recognition models (RetinaFace + ArcFace)
- [OpenAI Whisper](https://github.com/openai/whisper) - Speech-to-text model
- [ChromaDB](https://www.trychroma.com/) - Vector database for embeddings
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Python web framework
- [React](https://react.dev/) - UI library

### AI Services
- [Google Gemini](https://ai.google.dev/) - Context-aware AI assistant
- [Hugging Face](https://huggingface.co/) - Model hosting and transformers library

### Inspiration
- [Ray-Ban Meta Smart Glasses](https://www.meta.com/smart-glasses/) - Wearable computing platform

---

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/mindtrace/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/mindtrace/discussions)
- **Email**: support@mindtrace.dev

---

## üó∫Ô∏è Roadmap

### v1.1 (Q1 2025)
- [ ] Voice command activation ("Hey MindTrace")
- [ ] Offline face recognition mode
- [ ] Multi-language ASR support (Spanish, French, German)
- [ ] Mobile companion app (iOS/Android)

### v1.2 (Q2 2025)
- [ ] Apple Watch / WearOS integration
- [ ] Health tracking device integration (Fitbit, Apple Health)
- [ ] Advanced conversation analytics
- [ ] Custom wake word training

### v2.0 (Q3 2025)
- [ ] On-device ML inference (TensorFlow Lite)
- [ ] Federated learning for privacy
- [ ] Multi-user support for families
- [ ] Integration with smart home devices

---

<p align="center">
  <sub>Built with ‚ù§Ô∏è for those who need a little help remembering</sub>
</p>

<p align="center">
  <a href="#-mindtrace">‚¨Ü Back to Top</a>
</p>

---

**‚≠ê If you find MindTrace helpful, please consider giving it a star!**
