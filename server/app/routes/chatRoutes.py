"""
Chat API routes for AI-powered chatbot functionality.
Provides endpoints for sending messages and getting AI responses.
"""

import os
import uuid
from datetime import datetime
from typing import Optional, Dict, Any

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlalchemy.orm import Session

from ..database import get_db
from ..models import User
from ..utils.auth import get_current_user

router = APIRouter(
    prefix="/chat",
    tags=["chat"],
    responses={404: {"description": "Not found"}},
)


# Pydantic models
class ChatRequest(BaseModel):
    """Request model for chat messages."""
    message: str
    conversation_id: str
    context: Optional[Dict[str, Any]] = {}


class ChatResponse(BaseModel):
    """Response model for chat messages."""
    id: str
    content: str
    role: str = "assistant"
    timestamp: str
    streaming: bool = False


# In-memory conversation storage (for demo purposes)
# In production, you would store this in a database
conversations: Dict[str, list] = {}


def generate_ai_response(message: str, conversation_history: list) -> str:
    """
    Generate an AI response based on the user's message.
    
    This is a placeholder implementation. In production, you would:
    1. Connect to OpenAI/Anthropic API
    2. Send the conversation history for context
    3. Stream the response back
    
    Args:
        message: The user's message
        conversation_history: Previous messages for context
        
    Returns:
        The AI-generated response
    """
    # Placeholder responses based on keywords
    message_lower = message.lower()
    
    if "help" in message_lower:
        return """I'm your MindTrace AI Assistant! Here's what I can help you with:

**Contacts & Relationships:**
- View and manage your contacts
- Track relationships and interactions

**Reminders:**
- Set up daily reminders
- Check upcoming reminders

**Alerts:**
- View system notifications
- Check important updates

**General:**
- Navigate the dashboard
- Answer questions about the app

Just ask me anything, and I'll do my best to assist you! ðŸ˜Š"""

    if "reminder" in message_lower:
        return """Here's how to manage your reminders:

1. **View Reminders:** Go to Dashboard â†’ Reminders to see all your scheduled reminders
2. **Add New Reminder:** Click the "New Reminder" button in Quick Actions
3. **Edit/Delete:** Click on any reminder to modify or remove it

Would you like me to help you create a new reminder?"""

    if "contact" in message_lower:
        return """Here's how to manage your contacts:

1. **View Contacts:** Go to Dashboard â†’ Contacts Directory
2. **Add Contact:** Click "Add Contact" in Quick Actions
3. **Edit Contact:** Click on any contact card to view details and edit

You can also add photos for face recognition features! ðŸ“·"""

    if "status" in message_lower or "update" in message_lower:
        return """Here's your current status:

ðŸ”— **Glasses Status:** Connected
ðŸ”‹ **Battery:** 87%
â±ï¸ **Last Sync:** 2 minutes ago

Everything is running smoothly! Is there anything specific you'd like to know?"""

    if "sos" in message_lower or "emergency" in message_lower:
        return """**Emergency SOS Settings:**

Your SOS feature is configured and ready. In case of emergency:

1. **Activate SOS:** Triple-tap the glasses button
2. **Emergency Contacts:** Will be notified immediately
3. **Location Sharing:** Your location will be shared with contacts

âš ï¸ To configure SOS contacts, go to Dashboard â†’ SOS Settings"""

    # Default response
    return f"""I received your message: "{message}"

I'm here to help you navigate MindTrace! You can ask me about:
- Managing contacts and relationships
- Setting up reminders
- Viewing alerts and notifications
- Using the SOS emergency feature

What would you like to know more about?"""


@router.post("/", response_model=ChatResponse)
async def send_chat_message(
    request: ChatRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Send a chat message and receive an AI response.
    
    Args:
        request: The chat request containing the message
        db: Database session
        current_user: The authenticated user
        
    Returns:
        ChatResponse with the AI's reply
    """
    # Get or create conversation history
    conv_key = f"{current_user.id}_{request.conversation_id}"
    if conv_key not in conversations:
        conversations[conv_key] = []
    
    # Add user message to history
    conversations[conv_key].append({
        "role": "user",
        "content": request.message,
        "timestamp": datetime.utcnow().isoformat()
    })
    
    # Generate AI response
    ai_response = generate_ai_response(
        request.message, 
        conversations[conv_key]
    )
    
    # Add AI response to history
    conversations[conv_key].append({
        "role": "assistant", 
        "content": ai_response,
        "timestamp": datetime.utcnow().isoformat()
    })
    
    # Keep conversation history manageable (last 50 messages)
    if len(conversations[conv_key]) > 50:
        conversations[conv_key] = conversations[conv_key][-50:]
    
    return ChatResponse(
        id=str(uuid.uuid4()),
        content=ai_response,
        role="assistant",
        timestamp=datetime.utcnow().isoformat(),
        streaming=False
    )


async def generate_stream_response(message: str, conversation_history: list):
    """
    Generator for streaming AI responses.
    
    This is a placeholder for SSE streaming. In production, you would:
    1. Connect to OpenAI/Anthropic streaming API
    2. Yield chunks as they arrive
    """
    response = generate_ai_response(message, conversation_history)
    
    # Simulate streaming by yielding chunks
    words = response.split(' ')
    for i, word in enumerate(words):
        chunk = word + (' ' if i < len(words) - 1 else '')
        yield f"data: {chunk}\n\n"
        import asyncio
        await asyncio.sleep(0.05)  # Simulate network delay
    
    yield "data: [DONE]\n\n"


@router.post("/stream")
async def send_chat_message_streaming(
    request: ChatRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Send a chat message and receive a streaming AI response.
    Uses Server-Sent Events (SSE) for real-time streaming.
    
    Args:
        request: The chat request containing the message
        db: Database session
        current_user: The authenticated user
        
    Returns:
        StreamingResponse with SSE chunks
    """
    # Get or create conversation history
    conv_key = f"{current_user.id}_{request.conversation_id}"
    if conv_key not in conversations:
        conversations[conv_key] = []
    
    # Add user message to history
    conversations[conv_key].append({
        "role": "user",
        "content": request.message,
        "timestamp": datetime.utcnow().isoformat()
    })
    
    return StreamingResponse(
        generate_stream_response(request.message, conversations[conv_key]),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


@router.delete("/{conversation_id}")
async def clear_conversation(
    conversation_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Clear a conversation's history.
    
    Args:
        conversation_id: The conversation to clear
        db: Database session
        current_user: The authenticated user
        
    Returns:
        Success message
    """
    conv_key = f"{current_user.id}_{conversation_id}"
    if conv_key in conversations:
        del conversations[conv_key]
    
    return {"message": "Conversation cleared", "conversation_id": conversation_id}
